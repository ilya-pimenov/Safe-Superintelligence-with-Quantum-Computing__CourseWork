

## 1. How do you think the pace of takeoff will influence the design of AI safety systems?

In case of fast take-off it won't influence safety systems design, but will only "rely" on already existing safety systems, which we implemented beforehand.

In case of slow take-off, somehow, subjectively it seems we are always reactionary and just one -two steps behind; meaning that even of theoretically there would be time to implement AI safety systems, given slow take off it would be a challenging task to keep insisting on the necessity of ai safety systems. I feel the argument would be "but it is perfectly under controll right here", until it reaches the fast take-off speed.

## 2. In the context of slow vs. fast takeoff, which scenario seems more likely? Why?

Subjectively it feels that fast takeoff is more likely, since we seem to be reaching saturation point for transformer type of architecture. Comparing it to historical events of similar type we also mainly see fast takeoffs:

• It was 16 years between 15 kiloton explosion and 50 megaton
• Internet happened on us faster than people or businesses had time to adjust to it
• We still don't know how to adjust to social media as a global society

Altogether I struggle to come up with slow take off scenarios, we are essentially mostly reactive. 

## 3. What ethical considerations should be factored into AI systems during a fast takeoff? How can we ensure the AI aligns with human values quickly enough?

We need to embed human values into AI systems to begin with and make system capable of continuously understanding and updating them, as our values also continuosly evolve with the changing ecosystem; so that essentially in case of fast take-off we need to be able to co-exist with each other, taking into account that human values also change. Though deducing global human values seem to be a controversial task by itself; but it seems to be necessary as in a thought experimenet of reaching safe superintelligence, it will be global.

--

## 1. Discussion activity: Consider a scenario in which an AI system is tasked with maximizing happiness. What are the possible unintended consequences of such a directive? Discuss how you would approach alignment in this situation.

There is a broad spectrum of problems that such a statement may raise: in some form of trying to max out the value functions of amount of happy people per capita, reducing amount of non-happy people would raise that metric. In presense of guadrails against obvious disasters in of harming people, you could imagine such a system discovering new drugs (or combination of thereof) that make people "happy" as mesaured by data, yet since it could be a novel drug that we were not aware of previously super intelligence would not have data to support that it is harmfull; that would be connected to the issue of short term, mid term and long term happines and a choice what do you value more in a quest to maximize happinesss. And that breaches into philosopical question of whether happiness can exist in absence of sadness; inadverse effect of answering no to the previous question, can lead to a potential solution that would increase sadness greatly, in order to reach higher levels of hapiness as a function long term; which you might think would not be desireable.

I remember eating a slice of plain white bread in Saint-Petersburg when I was 9 or 10 years and thinking that it tastes absolutely amazing, simply due to the fact that I was extremely hungry.

## 2. Reflection: Think about a real-world example of AI or machine learning systems you've encountered. How might these systems benefit from more rigorous alignment practice? What steps could have been taken to prevent misalignment?

On the spot I immediately think about the ml systems which were made to approve credit scores. And surely they expressed a strong bias towards minorities, which, while potentially is true to the past collected data due to different socio-economic histories, doesn't benefit building a sustainable society tomorrow. There we attempts at "positive discrimination", which had other undersired effect (I guess those can be attributed to "Direct Specification" to the bigger machine) which were recently disputed. 

How this could've been avoided? This is a complex question. Yet usually it is easier to answer what we don't want the system to do; hence combination with Value Learning could help us design a framework of continuous Direct Specification of what we don't want. Like: Do we want to take applicant skin color or postcode into account when calculating credit-score? -> No.
