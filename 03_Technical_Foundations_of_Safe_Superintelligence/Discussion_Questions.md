
## If you were tasked with a self-improving AI system, what constraints and safeguards would you prioritize to ensure its safety and alignment?

Three come to mind:

• Having a gated environment
• Designing it in a way that the next improvement can be human audited
• Creating physical limitations where it can exist

--

## 1. What are the biggest challenges in predicting the behaviour of superintelligent systems?

Their "thinking" can look very different from ours and hence we woukdn't have "tools" to predict any of the effects that will manifest into our world.

## 2. How can simulation environments be improved to better reflect real-world conditions?

We could've put the simulation environments into a simulations environment and continue untill a meaningful amount of layers, which could be then physically isolated. That would allow us to see through how many layers the superintellingence will break, and layers can be designed in a way that each and next level presents heavier ethical challenges to break through.

For example to break through the first layers you have to lie and to break through further layers you have to inflict physical harm to people. That would also help us verify whether understanding of ethics that is built into the superintelligence holds correct.

## 3. In your opinion, what is the most critical component of a feedback loop in a superintelligent system?

A thread sleep timeout between the loops of feedback. :-D The mechanism that orchestrates these loops and doesn't let them spun out of control, as that allows to both ensure safety and potentially better learning process. 
