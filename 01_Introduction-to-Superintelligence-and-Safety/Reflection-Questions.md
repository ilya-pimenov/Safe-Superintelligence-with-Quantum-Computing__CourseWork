

## 1. What is a Paperclip Maximizer scenario, and how does it illustrate the AI alignment problem?

Paperclip Maximizer is a thought experiment in assumption we have a superintellingence (of I presume all three types: speed, space and quality) that if tasked with a benign "poc" question to see if works to "make me lots of paperclips" in absense of alignment with human values, the superintellingence will convert all resources and living matter into paperclips, having inadverse effect of destroying both people and the whole environment.

## 2. Why do you think that safete myst be the first priority in superintelligence development? Can you think of examples where a lack of foresight led to disastrous outcomes?

By its definition, if superintelligence is reached it is too late to try to make it safe, as in observing superintelligence unfolding we are already running a variation of Paperclip Maximizer experiment, just of a different sort.

On the spot I mainly think of examples where disastrous outcomes were received while acting in good faith, such as:

• Charities that brought clothing to Africa gave it out to kids in the poor neighbourhoods so that they had something to wear. But those clothing was seen as "fancy" by the local populace (branded work Nike, Adidas and etc.) and hence those same poor kids became targets to bullying and being robed in life-threatening situations for the clothers they received by charities.
• In Leningrad, after the German seige was lifted and food was brought into the city a number of people died due to their guts exploding as they couldn't mentally control eating due to their extreme state of malnutrition and trauma.
• Australia and all the rabbits, cats and insects they brought out of the best intentions and trying to do something without a meaningful foresight.

## 3. Discuss the concept of the "One-Chance Nature" of safe superintelligence design. Why is this such an important principle?

To me it feels imporant due to the notion, that given we will reach safe superintelligence it is not a given that we will understand how it "thinks", almost by definition we won't understand how exactly it does what we call "thinking". Akin to the question of "how many times can you try to do a nuclear explosion if you are trying to create a nuclear bomb in your bedroom". But in this case, the whole environment around us is a "bedroom" we are sitting in and the act of "nuclear explosion" can be not directly observed or comprehended and unfold in unclear timeline, that will make any attribution to our "experiment of creating superintelligence" impossible.
